# Kaggle_Chess_AI_Challenge
 
### 제약 조건
1. 5 MiB of RAM*
2. Dedicated CPU: a single 2.20GHz core
3. 64KiB compressed submission size limit
* RAM constraint is not exact. Expect an update which more strongly enforces this limit.

### 목표
 1. 강화학습을 사용
 2. 제약조건을 충족하는 모델 개발


### 여담
#### 2025.01.14 
 상당히 타이트한 용량, 메모리 제한을 지니고 있다. 일단 간단한 강화학습으로 가닥을 잡아야 할듯 일정이 빠듯하다.
 MCTS에 강화학습을 섞어서 해볼까 생각중이다. 될지는 모르겠음. 
 대회 참가에 의의를 두자. 쟁쟁한 사람들이 많다! 대회가 끝난 후 수상자들이 올린 설명들을 어서 읽어보고 싶다. 어떤 접근법을 썼을지 짐작조차 되지 않는다.

#### 2025.01.21
 근 일주일간 자취방에 갈일이 있어 최근 작업을 하지 못했다.(노트북을 가져가지 않았다.)
 MCTS를 사용해 테스트 에이전트를 만들어봤다.
 결과는 대실패!
 Simulation 단계에서 rollout를 piece-square tables로 대체했었는데, 잘 되지 않았다. 이해가 안되는 move를 많이 두는 경향이 있더라.
 최초에는 rollout을 수행하도록 만들었었지만, 시간도 오래걸렸을 뿐더러, 5 MiB 메모리로는 택도 없을 것 같았다.
 그래서 piece-square tables로 대체했던건데...
 어차피 강화학습으로 교체할 예정이긴 했지만, 이정도로 성능이 안나올줄은 예상하지 못했다.
 강화학습으로 학습시킨다 해도 좋은 성능이 나올지 확신이 서지 않는다. 알파고나 알파제로는 어떤식으로 구현했는지 논문을 읽어봐야 할듯.
 문제는 5 MiB 메모리 제한이다. 이걸 만족할 수 있을까.
 다른사람들은 어떻게 하고 있는건지 모르겠다. C, C++로 짠 후 포팅하는 방식을 취하고 있다고는 하는데, 성능을 생각하면 그게 맞는 것 같다.
 시간이 이젠 정말 얼마 남지 않았다.
 
 
#### 2025.01.22
 학습 가닥을 대충 잡았다. 일단 평가를 학습시킬 건데, MCTS가 아래 방식을 거치도록 하려함
 1. search
 2. select
 3. expand
 4. rollout 대신 신경망에 현재 state를 input으로 준 후 나온 output을 state와 함께 버퍼에 넣어둠.
 5. backpropagate
 이후 게임이 종료되면 버퍼에 넣어둔 값들의 result를 일괄적으로 적용한 후 한번에 update를 진행.
 문제는 역시 64kb 사이즈다.
 신경망을 50kb정도로 구성해야되는데, 최적화 기법들을 좀 많이 알아봐야 할듯... 이정도 신경망으로 학습이 제대로로 되기는 할까?
 첫 강화학습 프로젝트인데 너무 어려운 과제로 한게 아닌가 하는 생각도 좀 든다.
 그래도 난 한다면 하는 남자. 무슨일이 있어도 한다.


#### 2025.01.24
 지금 학습중이다. 모델 사이즈는 148kb다. 아직 최적화가 많이 필요함.
 지금 150번째 에피소드인데, 영 진전이 안보인다. 일단 1천까지는 지켜보고 어떻게 할지 정해야될듯
 Kaggle 환경 자체 버그인지, 승리가 아닌데도 승리했다고 뜨는 경우가 있다. 빈도도 꽤 높다. 10프로쯤 되는듯. 
 학습에 악영향을 줄 것 같은데 제발 큰 문제가 없기를.
  ㄴ[2025.01.26]
   환경을 건드린 적이 있었는데, 그게 문제였다. 현재는 정상 작동함


#### 2025.01.25
 학습을 다시 했다. 어제는 간단한 규칙 기반 봇을 상대로 학습을 수행했는데, 그리 좋지 않은 선택같았다.
 self-play로 수행할 생각이다. 이게 훨씬 나을듯.
 모델 사이즈는 양자화를 진행하니 53kb까지 줄어들었다. 잘하면 아슬아슬하게 64kb 안쪽이 될 것 같다.
 걱정되는건 ram이다. 라이브러리를 import할 수 있을지 의문이다. 안되면 비상이다. 내가 쓰는 부분만 따로 구현을 하고, 모델 사이즈도 줄여서 다시 학습해야한다.
 그리고 지금까지 cpu로 학습을 진행하고 있었다. 4080사놓고 이걸 안쓰고 있었네.
 문제는 tensorflow 2.11부터 윈도우에서 gpu를 지원 안한단다.
 2.10으로 다운그레이드 하거나, docker같은걸 써서 linux 환경에서 진행하거나. 어떻게든 해봐야겠다.
 2월 5일까지 11일밖에 안남았다. 이거 되나.


#### 2025.01.26
  이제 GPU를 사용한다. 학습이 200배 빨라졌다.
 파라미터를 늘렸다. 이번 턴에 두어야 할 색, 앙파상 가능 유무, 캐슬링 가능 유무, 50수 룰을을 추가했다. 기존에는 board의 상황만 input으로 주었었다.
 학습 방식도 조금 바꿨다. 리플레이 버퍼에 4096개의 표본이 쌓이면 학습을 진행하도록 했다. 원래는 정해진 에피소드마다 학습을 수행했었다. 
 현재 110 에피소드를 진행중이다. 흑이 무언가 확고한 전술을 배웠다. 폰을 끊임없이 앞으로 보내서 프로모션을 노리고 있다. 아직 체크메이트는 할 줄 몰라 50수 룰 혹은 스테일메이트로 인해 번번히 무승부를 기록중이다. 백은 진전이 없다. 이상하네.
 
 학습을 잠시 끊었다. 지금 self-play 방식에 문제가 많은 것 같다. 지금은 두 에이전트 모두 같은 정책을 쓴다. 에이전트2의 무브 역시 버퍼에 저장된다.(보상에는 -1을 곱함) 에이전트2는 스냅샷을 써서 학습에 관여하지 않도록 해야겠다. 지금 방식으로 하니까 학습이 너무 편향되는 느낌.
  